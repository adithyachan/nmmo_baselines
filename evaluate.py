import os
import json
import random
import logging
import argparse
from collections import defaultdict

import nmmo
import nmmo.core.config as nc

import pufferlib
import pufferlib.policy_pool as pp

from reinforcement_learning import clean_pufferl
import agent_zoo.neurips23_start_kit as default_learner

from train import get_init_args

NUM_AGENTS = 128
EVAL_TASK_FILE = 'neurips23_evaluation/sample_eval_task_with_embedding.pkl'
NUM_PVE_EVAL_EPISODE = 32
NUM_PVP_EVAL_EPISODE = 200  # TODO: cannot do more due to memory leak

def get_eval_config(debug=False):
    return {
        'device': 'cuda',
        'num_envs': 6 if not debug else 1,
        'batch_size': 2**15 if not debug else 2**12,
    }


class EvalConfig(nc.Medium, nc.Terrain, nc.Resource, nc.Combat, nc.NPC, nc.Progression,
                 nc.Item, nc.Equipment, nc.Profession, nc.Exchange):
    '''NMMO config for NeurIPS 2023 competition evaluation.
       Hardcoded to keep the eval config independent from the training config.
    '''
    def __init__(self, task_file, mode):
        super().__init__()
        self.set('GAME_PACKS', [(nmmo.core.game_api.AgentTraining, 1)])
        self.set('CURRICULUM_FILE_PATH', task_file)
        self.set('TASK_EMBED_DIM', 2048)  # must match the task file

        # Eval constants
        self.set('PROVIDE_ACTION_TARGETS', True)
        self.set('PROVIDE_NOOP_ACTION_TARGET', True)
        self.set('PLAYER_N', NUM_AGENTS)
        self.set('HORIZON', 1024)
        self.set('PLAYER_DEATH_FOG', None)
        self.set('NPC_N', 128)
        self.set('RESOURCE_RESILIENT_POPULATION', 0)
        self.set('COMBAT_SPAWN_IMMUNITY', 20)

        # Map related
        self.set('TERRAIN_FLIP_SEED', True)
        self.set('MAP_CENTER', 128)
        self.set('MAP_FORCE_GENERATION', False)
        self.set('MAP_GENERATE_PREVIEWS', True)
        if mode not in ['pve', 'pvp']:
            raise ValueError(f'Invalid eval_mode: {mode}')
        if mode == 'pve':
            self.set('MAP_N', 4)
            self.set('PATH_MAPS', 'maps/pve_eval/')
        else:
            self.set('MAP_N', 256)
            self.set('PATH_MAPS', 'maps/pvp_eval/')

def make_env_creator(task_file, mode):
    def env_creator(*args, **kwargs):  # dummy args
        env = nmmo.Env(EvalConfig(task_file, mode))
        # postprocessor is for the learner, which is not used in evaluation
        env = pufferlib.emulation.PettingZooPufferEnv(env,
            postprocessor_cls=default_learner.Postprocessor,
            postprocessor_kwargs={'eval_mode': True, 'early_stop_agent_num': 0,},
        )
        return env
    return env_creator

def make_agent_creator():
    policy_args = get_init_args(default_learner.Policy.__init__)
    def agent_creator(env):
        policy = default_learner.Policy(env, **policy_args)
        policy = pufferlib.frameworks.cleanrl.Policy(policy)
        return policy.to(get_eval_config()['device'])
    return agent_creator

class EvalRunner:
    def __init__(self, policy_store_dir, debug=False):
        self.policy_store_dir = policy_store_dir
        self._debug = debug

    def set_debug(self, debug):
        self._debug = debug

    def setup_evaluator(self, mode, task_file, seed):
        policies = pp.get_policy_names(self.policy_store_dir)
        assert len(policies) > 0, 'No policies found in eval_model_path'
        if mode == 'pve':
            assert len(policies) == 1, 'PvE mode requires only one policy'
        logging.info(f'Policies to evaluate: {policies}')

        # pool_kernel determines policy-agent mapping
        pool_kernel = pp.create_kernel(NUM_AGENTS, len(policies),
                                       shuffle_with_seed=seed)

        config = self.get_pufferl_config(self._debug)
        config.seed = seed
        config.data_dir = self.policy_store_dir
        config.pool_kernel = pool_kernel

        vectorization = pufferlib.vectorization.Serial if self._debug \
            else pufferlib.vectorization.Multiprocessing

        return clean_pufferl.create(
            config=config,
            agent_creator=make_agent_creator(),
            env_creator=make_env_creator(task_file, mode),
            vectorization=vectorization,
            eval_mode=True,
            eval_model_path=self.policy_store_dir,
            policy_selector=pp.AllPolicySelector(seed),
        )

    @staticmethod
    def get_pufferl_config(debug=False):
        config = get_eval_config(debug)
        # add required configs
        config['torch_deterministic'] = True
        config['total_timesteps'] = 100_000_000  # arbitrarily large, but will end much earlier
        config['envs_per_batch'] = config['num_envs']
        config['envs_per_worker'] = 1
        config['env_pool'] = False
        config['learning_rate'] = 1e-4
        config['compile'] = False
        config['verbose'] = True  # not debug
        return pufferlib.namespace(**config)

    def perform_eval(self, mode, task_file, seed, num_eval_episode, save_file_prefix):
        pufferl_data = self.setup_evaluator(mode, task_file, seed)
        # this is a hack
        pufferl_data.policy_pool.mask[:] = 1  # policy_pool.mask is valid for training only

        eval_results = {}
        cnt_episode = 0
        while cnt_episode < num_eval_episode:
            _, infos = clean_pufferl.evaluate(pufferl_data)

            for pol, vals in infos.items():
                cnt_episode += sum(infos[pol]['episode_done'])
                if pol not in eval_results:
                    eval_results[pol] = defaultdict(list)
                for k, v in vals.items():
                    if k == 'length':
                        eval_results[pol][k] += v  # length is a plain list
                    if k.startswith('curriculum'):
                        eval_results[pol][k] += [vv[0] for vv in v]

            pufferl_data.sort_keys = []  # TODO: check if this solves memory leak

            print(f'\nSeed: {seed}, evaluated {cnt_episode} episodes.\n')

        file_name = f'{save_file_prefix}_{seed}.json'
        self._save_results(eval_results, file_name)
        clean_pufferl.close(pufferl_data)
        return eval_results, file_name

    def _save_results(self, results, file_name):
        with open(os.path.join(self.policy_store_dir, file_name), 'w') as f:
            json.dump(results, f)

    def run(self, mode,
            task_file=EVAL_TASK_FILE,
            seed=None,
            num_episode=None,
            save_file_prefix=None):
        assert mode in ['pve', 'pvp'], f'Invalid mode: {mode}'
        if mode == 'pve':
            num_episode = num_episode or NUM_PVE_EVAL_EPISODE
            save_file_prefix = save_file_prefix or 'eval_pve'
        else:
            num_episode = num_episode or NUM_PVP_EVAL_EPISODE
            save_file_prefix = save_file_prefix or 'eval_pvp'

        if self._debug:
            num_episode = 4

        if seed is None:
            seed = random.randint(10000000, 99999999)

        logging.info(f'Evaluating {self.policy_store_dir} in the {mode} mode with seed: {seed}')
        logging.info(f'Using the task file: {task_file}')

        _, file_name = self.perform_eval(mode, task_file, seed, num_episode, save_file_prefix)

        print(f'Saved the result file to: {file_name}.')


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    parser = argparse.ArgumentParser(description='Evaluate a policy store')
    parser.add_argument('policy_store_dir', type=str, help='Path to the policy directory')
    parser.add_argument('mode', type=str, choices=['pve', 'pvp'], help='Evaluation mode')
    parser.add_argument('-t', '--task-file', type=str, default=EVAL_TASK_FILE, help='Path to the task file')
    parser.add_argument('-s', '--seed', type=int, default=1, help='Random seed')
    parser.add_argument('-n', '--num-episode', type=int, default=None, help='Number of episodes to evaluate')
    parser.add_argument('-r', '--repeat', type=int, default=1, help='Number of times to repeat the evaluation')
    parser.add_argument('--save-file-prefix', type=str, default=None, help='Prefix for the save file')
    parser.add_argument('--debug', action='store_true', help='Debug mode')
    args = parser.parse_args()

    runner = EvalRunner(args.policy_store_dir, args.debug)
    for i in range(args.repeat):
        if i > 0:
            args.seed = None  # this will sample new seed
        runner.run(args.mode, args.task_file, args.seed, args.num_episode, args.save_file_prefix)
